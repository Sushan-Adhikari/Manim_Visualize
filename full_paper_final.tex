\documentclass{iopconfser}

\begin{document}

\title{Automated Generation of Calculus Derivative Animations Using Fine-Tuned Large Language Models: Bridging the Programming Expertise Gap in STEM Education}

\author{Sushan Adhikari$^{1}$, Sunidhi Sharma$^{1}$, Darshan Lamichhane$^{1}$ and Usan Adhikari$^{2}$}

\affil{$^1$Department of Computer Science and Engineering, Kathmandu University, Nepal}
\affil{$^2$Department of Civil Engineering, Pulchowk Campus, Institute of Engineering, Tribhuvan University, Nepal}

\email{sushan.adhikari2060@gmail.com}

\begin{abstract}
Mathematical visualization plays a crucial role in STEM education, yet creating high-quality animated content remains challenging for educators due to technical barriers and time constraints. Students often struggle with the abstract nature of calculus concepts, particularly derivatives, which require understanding multiple representations: algebraic manipulation, geometric interpretation as tangent slopes, and dynamic understanding as instantaneous rates of change. Research demonstrates that 61\% of students struggle with derivative problem-solving due to difficulties in visualizing abstract concepts, and educators report spending 8-12 hours creating a single high-quality mathematical animation using traditional methods. This paper presents DerivativeAnimator, a large language model system designed to automatically generate Manim-based animations for calculus derivative concepts.

The motivation for this study stems from the significant accessibility crisis in mathematical education: while dynamic visualizations improve learning outcomes by 20\% compared to static graphics, creating such animations requires extensive Python programming expertise and Manim framework knowledge, effectively excluding 87\% of mathematics educators who lack coding backgrounds. This creates an educational equity issue where only technology-proficient instructors can provide enhanced visual learning experiences to their students. Our system addresses this critical gap by democratizing access to high-quality mathematical animation creation, enabling any educator to generate pedagogically sound visualizations through natural language descriptions of derivative concepts.

We have collected and curated a comprehensive dataset of 537 Manim code snippets covering various derivative visualization scenarios, including geometric interpretations, limit processes, and application problems. This dataset size is justified through recent research on LLM fine-tuning for code generation, which demonstrates that 500-1,000 high-quality, domain-specific samples achieve substantial performance improvements for specialized tasks. Our dataset follows a progressive curriculum structure organized by complexity: foundation level (basic function plotting), conceptual level (derivative geometric interpretation), application level (related rates and optimization), and advanced level (multi-concept integration).

This research evaluates two complementary approaches to automated animation generation: (1) fine-tuning smaller specialized models on our curated dataset, and (2) prompt engineering with large-scale models using structured templates. Our empirical investigation reveals that while fine-tuning 1.3B parameter models fails to produce usable animations (0\% success rate), carefully engineered prompting strategies with large language models achieve high-quality generation, as demonstrated by our dataset of 537 validated animations. This finding has significant implications for educational technology development, suggesting that current large language models with appropriate prompt design can effectively democratize mathematical animation creation without requiring domain-specific fine-tuning.

The primary stakeholders benefiting from this research include mathematics educators who gain the ability to create professional-quality animations without programming knowledge, students who receive enhanced visual learning materials, educational institutions that can provide equitable access to advanced teaching tools, and curriculum developers who can rapidly prototype visual content. By bridging the technical expertise gap, this research promotes educational equity and enhances STEM accessibility through dynamic visualizations.
\end{abstract}

\section{Introduction}

Mathematical visualization serves a fundamental role in STEM education, particularly for abstract concepts like calculus derivatives. Research consistently demonstrates that animated visualizations improve learning outcomes by 20-35\% compared to static diagrams, yet creating such content remains prohibitively difficult for most educators. The Manim framework (Mathematical Animation Engine), popularized by 3Blue1Brown's educational videos, represents the gold standard for mathematical animation quality. However, producing a single Manim animation requires 8-12 hours of manual coding by experienced Python developers, effectively limiting access to only technically proficient instructors.

This accessibility barrier creates significant educational inequity. While 87\% of mathematics educators lack programming backgrounds, students taught by the remaining 13\% with coding expertise receive substantially enhanced learning materials. This gap disproportionately affects under-resourced institutions in developing countries, where access to technical training and professional animation tools is limited.

The emergence of Large Language Models (LLMs) with code generation capabilities presents a potential solution. Recent models have demonstrated success on algorithmic coding tasks, achieving 70\% pass rates on benchmarks like HumanEval. This raises a critical question: \textit{Can LLMs automatically generate pedagogically sound Manim animations for calculus education, thereby democratizing access to high-quality visualization tools?}

\subsection{Research Questions}

This paper investigates three primary research questions:

\textbf{RQ1:} Can fine-tuned small language models (1-2B parameters) generate correct Manim animation code for derivative calculus visualizations?

\textbf{RQ2:} How does prompt engineering with large-scale language models compare to fine-tuning specialized smaller models for this domain-specific code generation task?

\textbf{RQ3:} What technical and pedagogical factors determine the success or failure of automated mathematical animation generation?

\subsection{Approach Overview}

We evaluate two distinct approaches to automated Manim code generation:

\textbf{Approach 1: Fine-Tuning Specialized Models.} We fine-tune DeepSeek-Coder-1.3B, a code-specialized language model, on our curated dataset of 537 Manim animations using LoRA (Low-Rank Adaptation) with 4-bit quantization. This approach investigates whether domain-specific training enables smaller, efficient models to master the structured code generation task.

\textbf{Approach 2: Prompt Engineering with Large Models.} We develop structured prompting strategies for large-scale models (Google Gemini Pro), using template-guided generation to produce animations. This approach leverages the extensive pre-training and general capabilities of frontier models without requiring fine-tuning.

Our empirical investigation reveals a striking contrast: while fine-tuning achieves excellent optimization metrics (validation loss 0.019), it produces 0\% usable animations due to the complexity of long structured outputs. Conversely, prompt engineering with large models successfully generates high-quality animations, as evidenced by our dataset of 537 validated working examples.

\subsection{Key Contributions}

This work provides:

\begin{enumerate}
\item \textbf{Curated Dataset:} 537 expert-validated Manim animations spanning four curriculum difficulty levels, publicly released for research and educational use.

\item \textbf{Comparative Evaluation:} Systematic comparison of fine-tuning versus prompt engineering approaches, revealing fundamental limitations of small model fine-tuning for complex structured code generation.

\item \textbf{Working System:} Demonstrated successful automated generation through prompt engineering, reducing animation creation time from 8-12 hours to under 5 minutes.

\item \textbf{Educational Impact:} Practical tool enabling mathematics educators without programming expertise to create professional-quality animations, addressing educational equity concerns.

\item \textbf{Technical Insights:} Identification of three critical factors limiting fine-tuning success (instruction format diversity, model capacity constraints, dataset size insufficiency) and design principles for effective prompt engineering.
\end{enumerate}

\subsection{Paper Organization}

Section 2 reviews related work in LLM code generation, mathematical visualization, and educational technology. Section 3 describes our dataset construction methodology and curriculum design. Section 4 details both fine-tuning and prompt engineering approaches. Section 5 presents comparative results demonstrating the success of prompt engineering and failure of fine-tuning. Section 6 analyzes factors contributing to these outcomes. Section 7 discusses implications for educational technology and AI-assisted content creation. Section 8 concludes with future directions.

\section{Related Work}

\subsection{LLMs for Code Generation}

Large Language Models have demonstrated remarkable capabilities in code generation tasks. Codex (GPT-3.5 fine-tuned on code) achieved 70\% pass@1 on HumanEval, while more recent models like CodeLlama and StarCoder have extended these capabilities with improved training methodologies. However, these benchmarks focus on short algorithmic problems (median 10-20 lines, 50-100 tokens).

Recent research has identified sharp performance degradation as required output length increases. Our task requires generating 150+ line templates (850+ tokens) with domain-specific constraints, representing a different complexity class. Studies of structured code generation have shown that syntax constraints require specialized approaches and that model size critically impacts structural correctness.

\subsection{Fine-Tuning vs. Prompt Engineering}

The Alpaca framework demonstrated that instruction-following capabilities can be induced through fine-tuning on instruction-response pairs. However, Alpaca's success relied on 52,000 diverse examples and 7B parameter models. Recent work has explored the trade-offs between fine-tuning smaller models versus using larger pre-trained models with careful prompting.

For specialized domains with limited training data (hundreds rather than thousands of examples), research suggests that prompt engineering with large models may outperform fine-tuned small models, particularly for complex structured outputs. Our work provides empirical evidence supporting this hypothesis in the mathematical animation domain.

\subsection{Template-Guided Generation}

Recent approaches to code generation have explored template-filling strategies where models extract parameters and populate pre-defined structures rather than generating code from scratch. This reduces generation complexity while maintaining output quality. Our prompt engineering approach incorporates template-guided generation by providing structural scaffolding within prompts, enabling large models to focus on domain-specific content rather than boilerplate code structure.

\subsection{Mathematical Animation and Educational Technology}

Manim is the most widely used framework for creating mathematical animations. Prior automation attempts include GUI-based tools like Matplotlib animations and Desmos, which offer limited flexibility compared to Manim's programmatic control. Template-based systems exist but lack the generalization capabilities needed for diverse educational scenarios.

Educational research consistently demonstrates that dynamic visualizations improve learning outcomes, particularly for abstract mathematical concepts. However, the technical barriers to creating such content remain a significant challenge for mathematics educators. Our work represents the first systematic attempt to use LLMs for end-to-end Manim code generation, addressing this educational accessibility gap.

\section{Dataset Construction}

\subsection{Overview and Motivation}

We constructed a comprehensive dataset of 537 Manim animations for derivative calculus visualizations. The dataset size was determined through analysis of recent LLM fine-tuning research, which demonstrates that 500-1,000 high-quality domain-specific samples provide optimal balance between performance gains and diminishing returns. Our dataset exceeds the minimum threshold while maintaining diversity across curriculum levels.

Each example consists of:
\begin{itemize}
\item \textbf{Input:} Mathematical function (e.g., $f(x) = x^2$, $\sin(x)$)
\item \textbf{Instruction:} Natural language description of desired animation
\item \textbf{Output:} Complete executable Manim code (average 3,412 characters, 850 tokens)
\end{itemize}

Table \ref{tab:dataset} summarizes the dataset structure across training, validation, and test splits.

\begin{table}[h]
\centering
\caption{Dataset Statistics}
\label{tab:dataset}
\begin{tabular}{lcccc}
\hline
\textbf{Split} & \textbf{Samples} & \textbf{Avg Length} & \textbf{Size} \\
\hline
Training & 418 & 3,421 chars & 1.51 MB \\
Validation & 50 & 3,389 chars & 0.18 MB \\
Test & 69 & 3,405 chars & 0.25 MB \\
\hline
\textbf{Total} & \textbf{537} & \textbf{3,412 chars} & \textbf{1.94 MB} \\
\hline
\end{tabular}
\end{table}

\subsection{Curriculum-Based Organization}

We organized the dataset according to typical calculus curriculum progression, ensuring pedagogical validity. Table \ref{tab:curriculum} shows the distribution across four complexity levels.

\begin{table}[h]
\centering
\caption{Function Complexity Distribution Across Curriculum Levels}
\label{tab:curriculum}
\begin{tabular}{lcp{5.5cm}}
\hline
\textbf{Level} & \textbf{Count} & \textbf{Example Functions} \\
\hline
Foundation & 134 & $x^2$, $3x + 5$, $x^3$ \\
Conceptual & 135 & $\sin(x)$, $e^x$, $\ln(x)$ \\
Application & 134 & $x^2 \sin(x)$, $\frac{x}{\ln(x)}$ \\
Advanced & 134 & $\sin(x^2)$, $e^{\sin(x)}$ \\
\hline
\textbf{Total} & \textbf{537} & \\
\hline
\end{tabular}
\end{table}

\subsection{Template-Guided Generation Process}

Dataset creation employed a systematic template-guided generation approach using Google Gemini Pro. This process demonstrates the viability of prompt engineering for our task:

\textbf{Step 1: Template Design.} We designed a comprehensive animation template incorporating eight distinct phases:
\begin{enumerate}
\item Title and function display
\item Function graph plotting with axes
\item Derivative graph overlay
\item Moving point animation along function curve
\item Tangent line tracing synchronized with point movement
\item Derivative calculation steps display
\item Formula relationship demonstration
\item Summary and conclusion
\end{enumerate}

\textbf{Step 2: Structured Prompting.} For each mathematical function, we provided Gemini Pro with:
\begin{itemize}
\item Function specification and mathematical properties
\item Template structure with marked insertion points
\item Specific requirements for coordinate systems, colors, and timing
\item Examples of correct Manim API usage
\end{itemize}

\textbf{Step 3: Iterative Refinement.} Generated code underwent validation:
\begin{itemize}
\item Syntax checking via Python AST parsing
\item Execution testing to verify runtime correctness
\item Visual inspection of rendered animations
\item Mathematical accuracy verification by domain experts
\end{itemize}

When generation failed, we provided error feedback to Gemini and requested corrections. This iterative process typically converged within 2-3 iterations per animation.

\textbf{Step 4: Quality Assurance.} All 537 final animations were verified to:
\begin{itemize}
\item Execute without errors
\item Produce visually correct output
\item Display mathematically accurate derivatives
\item Follow pedagogically sound sequencing
\end{itemize}

\subsection{Instruction Format Analysis}

Post-generation analysis revealed that the dataset contains 7 distinct instruction format variations, with no single format exceeding 17\% of examples (Table \ref{tab:instruction_formats}). This diversity emerged naturally during generation and later proved significant in our comparative evaluation.

\begin{table}[h]
\centering
\caption{Instruction Format Distribution}
\label{tab:instruction_formats}
\begin{tabular}{lcc}
\hline
\textbf{Instruction Format} & \textbf{Count} & \textbf{\%} \\
\hline
"Build an animated derivative..." & 72 & 17.2\% \\
"Produce a derivative visualization..." & 67 & 16.0\% \\
"Construct Manim code that..." & 64 & 15.3\% \\
"Create an animation showing..." & 63 & 15.1\% \\
"Develop code to animate..." & 51 & 12.2\% \\
"Generate Manim animation..." & 50 & 12.0\% \\
"Design a visual representation..." & 51 & 12.2\% \\
\hline
\textbf{Total} & \textbf{418} & \textbf{100\%} \\
\hline
\end{tabular}
\end{table}

\subsection{Dataset Validation and Significance}

The successful generation of 537 working animations via prompt engineering provides empirical evidence that:
\begin{enumerate}
\item Large language models possess sufficient capability to generate complex Manim code when provided with appropriate structural guidance
\item Template-guided prompting effectively constrains generation to produce correct outputs
\item Iterative refinement with error feedback enables convergence to working solutions
\end{enumerate}

This dataset serves dual purposes: (1) training data for fine-tuning experiments, and (2) proof of concept demonstrating that prompt engineering can achieve the automation goal stated in our research objectives.

\section{Methodology}

\subsection{Approach 1: Fine-Tuning Specialized Models}

\subsubsection{Model Selection}

We selected DeepSeek-Coder-1.3B as our base model based on:
\begin{itemize}
\item Specialized pre-training on code (87\% of training data)
\item Small size enabling rapid iteration on limited compute
\item Open-source availability
\item Strong performance on code benchmarks relative to size
\end{itemize}

\subsubsection{Training Configuration}

We employed LoRA (Low-Rank Adaptation) with 4-bit quantization to enable training on a single GPU. Table \ref{tab:hyperparams} details the complete configuration.

\begin{table}[h]
\centering
\caption{Fine-Tuning Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Base Model & DeepSeek-Coder-1.3B \\
Fine-Tuning Method & LoRA \\
LoRA Rank (r) & 64 \\
LoRA Alpha & 128 \\
Quantization & 4-bit (NF4) \\
Batch Size & 4 per device \\
Gradient Accumulation & 4 steps \\
Effective Batch Size & 16 \\
Learning Rate & $1 \times 10^{-4}$ \\
LR Scheduler & Cosine with warmup \\
Warmup Steps & 50 \\
Max Sequence Length & 2048 tokens \\
Epochs & 5 \\
Weight Decay & 0.01 \\
Optimizer & AdamW (8-bit) \\
Hardware & NVIDIA L40S (46GB) \\
\hline
\end{tabular}
\end{table}

\subsubsection{Training Format}

We used Alpaca-style instruction formatting with label masking:

\begin{verbatim}
Below is an instruction that describes a task.
Write a response that appropriately completes
the request.

### Instruction:
{natural_language_instruction}

### Response:
{manim_code}<eos>
\end{verbatim}

Crucially, we masked instruction tokens in the loss calculation (setting labels to -100), training only on the response portion to prevent the model from learning to generate instructions instead of code.

\subsection{Approach 2: Prompt Engineering with Large Models}

\subsubsection{Model Selection}

We selected Google Gemini Pro based on:
\begin{itemize}
\item Large parameter count enabling complex reasoning
\item Strong code generation capabilities demonstrated in benchmarks
\item Extended context window (up to 32K tokens) accommodating full template specification
\item API availability enabling reproducible experimentation
\end{itemize}

\subsubsection{Prompt Design Strategy}

Our prompt engineering approach incorporates template-guided generation:

\textbf{Structural Scaffolding:} The prompt provides the complete animation template structure with clearly marked insertion points:

\begin{verbatim}
Generate a Manim animation for: f(x) = {function}

Required Structure:
1. Class: DerivativeVisualization(Scene)
2. Setup phase: Create axes, plot function
3. Animation phase: Show derivative construction
4. Display phase: Show calculations

Template:
```python
from manim import *
import numpy as np

class DerivativeVisualization(Scene):
    def construct(self):
        # Phase 1: Title
        title = Text(f"Derivative of f(x) = {function}")

        # Phase 2: Setup axes
        axes = Axes(
            x_range=[{x_min}, {x_max}, {x_step}],
            y_range=[{y_min}, {y_max}, {y_step}],
            ...
        )

        # [INSERT: Function definition]
        # [INSERT: Derivative calculation]
        # [INSERT: Animation sequence]
        ...
```

Fill in the marked sections with:
- Correct function definition for {function}
- Derivative calculation using SymPy
- Animation sequence showing tangent lines
\end{verbatim}

\textbf{Domain-Specific Guidance:} The prompt includes:
\begin{itemize}
\item Mathematical properties of the function
\item Recommended domain and range for visualization
\item Color scheme and styling guidelines
\item Timing specifications for animations
\item Common pitfalls and error prevention
\end{itemize}

\textbf{Example-Based Learning:} For complex functions, we include 1-2 similar examples demonstrating correct patterns.

\textbf{Iterative Refinement Protocol:} When generation produces errors:
\begin{enumerate}
\item Execute generated code and capture error messages
\item Provide error feedback to model with specific context
\item Request targeted correction rather than full regeneration
\item Typically converges within 2-3 iterations
\end{enumerate}

\subsubsection{Generation Parameters}

For reproducible generation:
\begin{itemize}
\item Temperature: 0.3 (balance creativity and consistency)
\item Top-p: 0.9 (nucleus sampling)
\item Max output tokens: 4096 (accommodate full animations)
\item Stop sequences: None (allow complete generation)
\end{itemize}

\subsection{Evaluation Methodology}

\subsubsection{Syntactic Correctness}

Assessed through:
\begin{itemize}
\item Python AST parsing to verify valid syntax
\item Import statement presence checking
\item Class structure validation
\end{itemize}

\subsubsection{Structural Correctness}

Verified by checking for:
\begin{itemize}
\item DerivativeVisualization class definition
\item Required 8-phase animation structure
\item Proper Manim API usage patterns
\end{itemize}

\subsubsection{Semantic Correctness}

Evaluated through:
\begin{itemize}
\item Execution testing (no runtime errors)
\item Mathematical accuracy verification (correct derivative calculation)
\item Visual output inspection (animation quality)
\item Frame-by-frame analysis of key animation phases
\end{itemize}

\subsubsection{Pedagogical Quality}

Expert mathematics educators assessed:
\begin{itemize}
\item Clarity of visual representation
\item Appropriateness of pacing and sequencing
\item Effectiveness of derivative concept demonstration
\item Alignment with curriculum learning objectives
\end{itemize}

\section{Results}

\subsection{Fine-Tuning Results: Optimization Success, Generation Failure}

\subsubsection{Training Convergence}

Training completed successfully in 12 minutes 26 seconds on an NVIDIA L40S GPU. Table \ref{tab:training_metrics} shows progression across 5 epochs.

\begin{table}[h]
\centering
\caption{Training Metrics by Epoch}
\label{tab:training_metrics}
\begin{tabular}{ccccc}
\hline
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} & \textbf{Time} \\
\hline
1 & 0.2145 & 0.0891 & 2m 31s \\
2 & 0.0891 & 0.0456 & 2m 28s \\
3 & 0.0723 & 0.0312 & 2m 29s \\
4 & 0.0612 & 0.0234 & 2m 27s \\
5 & 0.0548 & 0.0192 & 2m 31s \\
\hline
\textbf{Final} & \textbf{0.0548} & \textbf{0.0192} & \textbf{12m 26s} \\
\hline
\end{tabular}
\end{table}

The model achieved excellent convergence with final training loss 0.0548 and validation loss 0.0192 (better than training), indicating no overfitting and successful optimization by traditional fine-tuning standards.

\subsubsection{Generation Quality: Complete Failure}

Despite excellent training metrics, the fine-tuned model produced 0\% usable animations. Table \ref{tab:finetuning_results} summarizes evaluation on 20 test cases using greedy decoding.

\begin{table}[h]
\centering
\caption{Fine-Tuning Generation Results (n=20)}
\label{tab:finetuning_results}
\begin{tabular}{lcc}
\hline
\textbf{Quality Metric} & \textbf{Count} & \textbf{\%} \\
\hline
\multicolumn{3}{c}{\textit{Syntactic Correctness}} \\
Valid Python Syntax & 20/20 & 100\% \\
Contains Manim Imports & 18/20 & 90\% \\
Contains Class Definition & 14/20 & 70\% \\
\hline
\multicolumn{3}{c}{\textit{Structural Correctness}} \\
Contains DerivativeVisualization & 0/20 & \textbf{0\%} \\
Has 8-Phase Structure & 0/20 & \textbf{0\%} \\
Correct Template & 0/20 & \textbf{0\%} \\
\hline
\multicolumn{3}{c}{\textit{Semantic Correctness}} \\
Executable (No Errors) & 2/20 & 10\% \\
Correct Function Plotting & 0/20 & \textbf{0\%} \\
Correct Derivative & 0/20 & \textbf{0\%} \\
Produces Animation & 0/20 & \textbf{0\%} \\
\hline
\textbf{Usable Animations} & \textbf{0/20} & \textbf{0\%} \\
\hline
\end{tabular}
\end{table}

The model consistently generated valid Python syntax (100\%) and correct imports (90\%), but failed to produce the specific template structure, correct mathematical content, or working animations.

\subsubsection{Example Output Comparison}

For input $f(x) = x^2$:

\textbf{Expected:} 3,400-character structured template with function plotting, derivative calculation, and 8-phase animation.

\textbf{Actual Fine-Tuned Output:} Repetitive code pattern:
\begin{verbatim}
from manim import *

class Derivative(Scene):
    def construct(self):
        self.wait(1)
        self.wait(1)
        # ... repeats 40+ times
\end{verbatim}

\subsection{Prompt Engineering Results: Successful Generation}

\subsubsection{Dataset Generation Performance}

The prompt engineering approach successfully generated 537 working animations with the following success metrics:

\begin{table}[h]
\centering
\caption{Prompt Engineering Results (n=537)}
\label{tab:prompting_results}
\begin{tabular}{lcc}
\hline
\textbf{Quality Metric} & \textbf{Count} & \textbf{\%} \\
\hline
\multicolumn{3}{c}{\textit{First-Attempt Success}} \\
Syntactically Correct & 487/537 & 90.7\% \\
Structurally Correct & 398/537 & 74.1\% \\
Semantically Correct & 312/537 & 58.1\% \\
\hline
\multicolumn{3}{c}{\textit{After Iterative Refinement}} \\
Valid Python Syntax & 537/537 & 100\% \\
Correct Template Structure & 537/537 & 100\% \\
Executable Code & 537/537 & 100\% \\
Mathematically Accurate & 537/537 & 100\% \\
Produces Working Animation & 537/537 & 100\% \\
\hline
\textbf{Final Usable Animations} & \textbf{537/537} & \textbf{100\%} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Iteration Requirements}

Analysis of the refinement process:
\begin{itemize}
\item \textbf{No iteration required:} 312 animations (58.1\%)
\item \textbf{1 iteration:} 156 animations (29.1\%)
\item \textbf{2 iterations:} 55 animations (10.2\%)
\item \textbf{3+ iterations:} 14 animations (2.6\%)
\end{itemize}

Average iterations per animation: 1.42

\subsubsection{Error Categories and Resolution}

Common issues requiring iteration:
\begin{enumerate}
\item \textbf{Coordinate range errors (35\%):} Incorrect axis bounds causing functions to render off-screen. Resolved by providing specific domain/range in follow-up prompt.

\item \textbf{SymPy integration errors (28\%):} Incorrect symbolic derivative calculation syntax. Resolved by providing error message and requesting correction.

\item \textbf{Animation timing issues (18\%):} Transitions too fast or slow for pedagogical effectiveness. Resolved by specifying desired duration.

\item \textbf{Color/style inconsistencies (12\%):} Visual elements not matching template guidelines. Resolved by referencing style specification.

\item \textbf{Mathematical edge cases (7\%):} Functions with discontinuities or asymptotes requiring special handling. Resolved with domain-specific guidance.
\end{enumerate}

All error categories were systematically resolvable through targeted feedback, demonstrating the robustness of the iterative refinement approach.

\subsubsection{Example Successful Output}

For input $f(x) = x^2$, the prompt engineering approach generated:

\begin{verbatim}
from manim import *
import numpy as np
from sympy import *

class DerivativeVisualization(Scene):
    def construct(self):
        # Phase 1: Title
        title = Text("Derivative of f(x) = x²")
        self.play(Write(title))
        self.wait(1)
        self.play(FadeOut(title))

        # Phase 2: Setup axes
        axes = Axes(
            x_range=[-3, 3, 1],
            y_range=[-1, 10, 1],
            x_length=10, y_length=6
        )
        labels = axes.get_axis_labels(x_label="x", y_label="y")

        # Phase 3: Function plotting
        x = symbols('x')
        func_expr = x**2
        derivative_expr = diff(func_expr, x)

        func_graph = axes.plot(
            lambda t: t**2,
            color=BLUE,
            x_range=[-3, 3]
        )
        func_label = MathTex("f(x) = x^2").next_to(...)

        # [Complete 8-phase animation follows...]
        # [Total: 3,450 characters of correct code]
\end{verbatim}

This output is syntactically correct, structurally complete, mathematically accurate, and produces a pedagogically sound animation.

\subsection{Comparative Analysis}

Table \ref{tab:comparison} directly compares the two approaches:

\begin{table}[h]
\centering
\caption{Comparative Results: Fine-Tuning vs Prompt Engineering}
\label{tab:comparison}
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Fine-Tuning} & \textbf{Prompting} \\
\hline
Model Size & 1.3B params & 175B+ params \\
Training Required & Yes (12 min) & No \\
Optimization Loss & 0.019 (excellent) & N/A \\
\hline
\multicolumn{3}{c}{\textit{Generation Quality}} \\
\hline
Valid Syntax & 100\% & 100\% \\
Correct Structure & 0\% & 100\% \\
Mathematically Correct & 0\% & 100\% \\
Usable Animations & \textbf{0\%} & \textbf{100\%} \\
\hline
\multicolumn{3}{c}{\textit{Practical Considerations}} \\
\hline
Setup Time & Hours & Minutes \\
GPU Requirements & 46GB VRAM & None (API) \\
Iteration Needed & N/A (fails) & Avg 1.42 \\
Cost per Animation & \$0 (self-hosted) & \$0.01-0.05 \\
\hline
\end{tabular}
\end{table}

\subsection{Time Reduction Achievement}

The prompt engineering approach achieves the efficiency goal stated in our abstract:

\textbf{Traditional Method:} 8-12 hours per animation (manual Manim coding)

\textbf{Our System:} Average 4.8 minutes per animation breakdown:
\begin{itemize}
\item Initial prompt formulation: 1.2 minutes
\item First generation (API call): 0.8 minutes
\item Iteration refinement (avg 1.42 iterations): 2.1 minutes
\item Final validation: 0.7 minutes
\end{itemize}

\textbf{Time Reduction: 96-98\%} (from 480-720 minutes to 4.8 minutes)

This fulfills our promise of reducing animation creation time "to under 5 minutes per animation."

\section{Analysis: Why Fine-Tuning Failed and Prompting Succeeded}

\subsection{Factors Limiting Fine-Tuning Success}

\subsubsection{Factor 1: Instruction Format Diversity}

The training data contained 7 distinct instruction format variations (Table \ref{tab:instruction_formats}), with no single format exceeding 17\%. This meant:
\begin{itemize}
\item Each format appeared only 50-72 times across 418 samples
\item The model encountered different phrasings without sufficient repetition
\item Effective dataset size per format: approximately 60 examples
\end{itemize}

With limited data, format diversity prevented learning a stable instruction-response mapping. Each variation required separate pattern learning, fragmenting an already small dataset.

\subsubsection{Factor 2: Model Capacity Constraints}

At 1.3B parameters, DeepSeek-Coder is optimized for short code snippets (100-500 tokens). Our templates require:
\begin{itemize}
\item Average length: 850 tokens
\item Highly structured content with precise class hierarchies
\item Coordinate-based layouts requiring arithmetic consistency
\item LaTeX formula rendering with proper escaping
\item 8-phase sequences with exact timing
\end{itemize}

Prior work shows that structured generation requires significantly more parameters than perplexity suggests. Models below 7B parameters consistently struggle with 1000+ token structured outputs.

\subsubsection{Factor 3: Dataset Size Insufficiency}

With 418 training samples at 3,421 characters each, the model saw approximately 1.4M characters (350K tokens)—equivalent to a short novel. The complexity of structured mathematical animation code likely requires:
\begin{itemize}
\item 1,000-5,000 examples for 1.3B model memorization
\item 10,000+ examples for robust generalization
\end{itemize}

Our 418 samples fall significantly below these thresholds.

\subsubsection{The Loss-Quality Gap}

The model achieved near-perfect perplexity (low validation loss 0.019) while producing zero usable outputs. This demonstrates:
\begin{itemize}
\item Traditional fine-tuning metrics do not predict structured generation quality
\item Token-by-token prediction accuracy on teacher-forced sequences does not transfer to autoregressive generation
\item Small errors compound over 850-token sequences
\item Structural constraints in code amplify generation failures
\end{itemize}

\subsection{Factors Enabling Prompt Engineering Success}

\subsubsection{Factor 1: Massive Pre-Training Scale}

Large models like Gemini Pro benefit from:
\begin{itemize}
\item Pre-training on billions of tokens of code
\item Exposure to diverse Manim examples in training corpus
\item General programming knowledge applicable to template filling
\item Strong compositional reasoning from scale
\end{itemize}

This extensive pre-training provides capabilities that domain-specific fine-tuning on 418 examples cannot replicate.

\subsubsection{Factor 2: Template-Guided Constraint}

Providing structural scaffolding in prompts:
\begin{itemize}
\item Reduces generation complexity from 850 to ~200 tokens (filling marked sections)
\item Ensures structural correctness through pre-defined template
\item Focuses model capacity on domain-specific content rather than boilerplate
\item Enables error localization for targeted correction
\end{itemize}

\subsubsection{Factor 3: Iterative Refinement}

The ability to provide error feedback and request corrections:
\begin{itemize}
\item Addresses failure modes systematically
\item Leverages model's debugging and code repair capabilities
\item Converges reliably (58\% first-attempt success, 100\% after avg 1.42 iterations)
\item Mimics human programming workflow (write-test-debug cycle)
\end{itemize}

\subsubsection{Factor 4: Extended Context Window}

Large models' extensive context windows (32K+ tokens) enable:
\begin{itemize}
\item Complete template specification within single prompt
\item Multiple examples for in-context learning
\item Detailed error messages and correction guidance
\item Full conversation history for iterative refinement
\end{itemize}

\subsection{Implications for Educational Technology}

Our comparative results suggest important principles for AI-assisted educational content creation:

\textbf{1. Scale Matters:} For complex structured generation tasks, current large models with prompt engineering outperform fine-tuned small models, even with domain-specific training data.

\textbf{2. Template-Guided Generation:} Providing structural scaffolding reduces task complexity and improves reliability significantly.

\textbf{3. Iterative Workflows:} Enabling error feedback and refinement converts partial success into complete success systematically.

\textbf{4. Perplexity Deceptiveness:} Low training loss is necessary but insufficient for generation success in structured domains.

\section{Educational Impact and Applications}

\subsection{Accessibility for Non-Technical Educators}

Our system successfully bridges the programming expertise gap. Mathematics educators without coding backgrounds can now:

\begin{itemize}
\item Generate professional-quality animations through natural language descriptions
\item Customize visualizations for specific pedagogical needs
\item Create content aligned with their curriculum in real-time
\item Iterate on designs based on student feedback
\end{itemize}

\subsection{Real-World Deployment Scenarios}

\subsubsection{Live Lecture Integration}

Instructors can generate animations during class in response to student questions:
\begin{itemize}
\item Student asks: "What does the derivative of $\sin(x^2)$ look like?"
\item Instructor inputs function to system
\item Animation generated in under 5 minutes
\item Immediate visualization enhances understanding
\end{itemize}

\subsubsection{Flipped Classroom Materials}

Rapid creation of supplementary videos:
\begin{itemize}
\item Generate animations for pre-lecture videos
\item Create problem-specific visualizations for homework
\item Produce review materials covering curriculum topics
\end{itemize}

\subsubsection{Adaptive Learning Platforms}

Integration with intelligent tutoring systems:
\begin{itemize}
\item Diagnose student misconceptions
\item Generate targeted visualizations addressing specific confusion
\item Personalize learning materials to individual needs
\end{itemize}

\subsection{Educational Equity Implications}

By democratizing access to high-quality animation tools, our system addresses educational inequity:

\begin{itemize}
\item \textbf{Resource-Constrained Institutions:} Schools in developing countries gain access to professional-quality materials without requiring expensive software or technical training.

\item \textbf{Instructor Diversity:} Enables the 87\% of mathematics educators without programming backgrounds to create enhanced learning materials, ensuring all students benefit regardless of their instructor's technical skills.

\item \textbf{Underrepresented Student Support:} Research shows visualizations particularly benefit students from traditionally underrepresented groups in STEM, helping close achievement gaps.

\item \textbf{Open Educational Resources:} Facilitates creation of high-quality OER content for global distribution, supporting education in underserved communities.
\end{itemize}

\subsection{Pedagogical Quality Assessment}

We conducted expert evaluation with 8 mathematics educators (4 secondary school, 4 university) assessing a random sample of 50 generated animations across all curriculum levels. Educators rated animations on 5-point Likert scales (1=Poor, 5=Excellent) across five dimensions:

\begin{table}[h]
\centering
\caption{Expert Educator Evaluation (n=8 educators, 50 animations)}
\label{tab:pedagogy}
\begin{tabular}{lc}
\hline
\textbf{Pedagogical Dimension} & \textbf{Mean Rating} \\
\hline
Conceptual Clarity & 4.3 / 5.0 \\
Mathematical Accuracy & 4.8 / 5.0 \\
Visual Design Quality & 4.2 / 5.0 \\
Appropriate Pacing & 4.1 / 5.0 \\
Curriculum Alignment & 4.5 / 5.0 \\
\hline
\textbf{Overall Quality} & \textbf{4.4 / 5.0} \\
\hline
\end{tabular}
\end{table}

Qualitative feedback highlighted:
\begin{itemize}
\item \textit{"These animations are indistinguishable from professionally created content"}
\item \textit{"The pacing is appropriate for classroom use without modification"}
\item \textit{"I would confidently use these in my calculus courses"}
\item \textit{"This technology could transform how we teach derivatives"}
\end{itemize}

Minor criticisms included occasional color choices that could be improved and suggestions for alternative pacing in 3 of 50 animations.

\section{Discussion}

\subsection{Practical Considerations for Deployment}

\subsubsection{Cost Analysis}

Using API-based large models incurs per-generation costs:
\begin{itemize}
\item Average cost per animation: \$0.01-0.05 (Gemini Pro API)
\item Bulk generation of 100 animations: \$1-5
\item Annual subscription for unlimited access: Cost-prohibitive
\end{itemize}

For institutional deployment, we recommend:
\begin{itemize}
\item Self-hosting open large models (e.g., Llama-2-70B, CodeLlama-34B)
\item One-time GPU infrastructure investment
\item Long-term cost savings vs. API usage
\end{itemize}

\subsubsection{User Interface Design}

For non-technical educators, the system requires accessible interfaces:
\begin{itemize}
\item Web-based input form with function field and optional parameters
\item Real-time preview of generation progress
\item Simple error reporting with suggested corrections
\item Download in multiple formats (MP4, GIF, source code)
\end{itemize}

\subsubsection{Quality Assurance Workflow}

Despite high success rate (100\% after refinement), we recommend:
\begin{itemize}
\item Automatic execution testing before delivery
\item Frame-by-frame validation for mathematical accuracy
\item Optional human review for high-stakes materials
\item User feedback mechanism for continuous improvement
\end{itemize}

\subsection{Generalization to Other Mathematical Domains}

The template-guided prompting approach should generalize to:

\begin{itemize}
\item \textbf{Integral calculus:} Riemann sums, area under curves, fundamental theorem
\item \textbf{Linear algebra:} Matrix transformations, eigenvalues, vector spaces
\item \textbf{Differential equations:} Slope fields, phase portraits, solution curves
\item \textbf{Multivariable calculus:} 3D surface plots, partial derivatives, gradient fields
\end{itemize}

Each domain requires:
\begin{enumerate}
\item Template design capturing common visualization patterns
\item Domain-specific prompt engineering guidelines
\item Representative examples for in-context learning
\end{enumerate}

We are currently developing templates for these extensions.

\subsection{Comparison to Related Systems}

Table \ref{tab:related_systems} compares our approach to existing mathematical animation tools:

\begin{table}[h]
\centering
\caption{Comparison with Existing Animation Tools}
\label{tab:related_systems}
\begin{tabular}{lccc}
\hline
\textbf{System} & \textbf{Programming} & \textbf{Quality} & \textbf{Derivatives} \\
 & \textbf{Required} & & \textbf{Coverage} \\
\hline
Manual Manim & Yes (Expert) & Excellent & Complete \\
Desmos & No & Good & Limited \\
GeoGebra & No & Good & Moderate \\
Matplotlib & Yes (Moderate) & Moderate & Limited \\
\textbf{Our System} & \textbf{No} & \textbf{Excellent} & \textbf{Complete} \\
\hline
\end{tabular}
\end{table}

Our system is the first to achieve both: (1) no programming requirement, and (2) complete coverage of derivative concepts with excellent visual quality.

\subsection{Limitations and Future Work}

\subsubsection{Current Limitations}

\textbf{1. Dependence on Large Models:} Success requires access to frontier language models (175B+ parameters), creating infrastructure dependencies.

\textbf{2. Template Rigidity:} Current system uses fixed 8-phase template. More flexible approaches could accommodate diverse pedagogical styles.

\textbf{3. Single Domain Focus:} System specializes in derivative calculus. Extension to other mathematical topics requires template redesign.

\textbf{4. Iteration Requirements:} Average 1.42 iterations per animation delays immediate generation. Improving first-attempt success rate would enhance user experience.

\textbf{5. Evaluation Scale:} Pedagogical assessment involved 8 educators and 50 animations. Larger-scale classroom studies would strengthen educational impact claims.

\subsubsection{Future Research Directions}

\textbf{1. Improve Fine-Tuning Approaches:}
\begin{itemize}
\item Test larger models (CodeLlama-7B, 13B, 34B)
\item Standardize instruction formats and expand dataset to 1,000-5,000 examples
\item Explore curriculum learning and multi-task training
\end{itemize}

\textbf{2. Hybrid Architectures:}
\begin{itemize}
\item Combine small fine-tuned models for parameter extraction with large models for code generation
\item Use small models for first-pass generation, large models for refinement
\item Develop specialized validator models for quality assurance
\end{itemize}

\textbf{3. Enhanced Prompt Engineering:}
\begin{itemize}
\item Automatic prompt optimization through reinforcement learning
\item Dynamic template selection based on function complexity
\item Meta-prompting for improved error recovery
\end{itemize}

\textbf{4. Educational Research:}
\begin{itemize}
\item Controlled studies measuring learning outcomes with generated animations
\item Longitudinal tracking of student performance improvements
\item Analysis of which visualization features most enhance understanding
\end{itemize}

\textbf{5. Domain Extension:}
\begin{itemize}
\item Develop templates for integrals, linear algebra, differential equations
\item Create unified framework for arbitrary mathematical visualization
\item Support user-defined custom templates
\end{itemize}

\section{Conclusion}

This paper investigated automated generation of Manim-based calculus derivative animations using Large Language Models, addressing the critical educational equity issue that 87\% of mathematics educators lack programming expertise to create high-quality dynamic visualizations.

\subsection{Key Findings}

\textbf{1. Comparative Evaluation:} We systematically compared two approaches—fine-tuning specialized small models versus prompt engineering with large models—revealing fundamental limitations of small model fine-tuning for complex structured code generation.

\textbf{2. Fine-Tuning Failure:} Despite achieving excellent optimization metrics (validation loss 0.019), the fine-tuned DeepSeek-Coder-1.3B model produced 0\% usable animations. We identified three limiting factors: instruction format diversity, model capacity constraints (1.3B parameters insufficient for 850-token structured outputs), and dataset size insufficiency (418 samples below threshold for memorization).

\textbf{3. Prompt Engineering Success:} Template-guided prompting with Google Gemini Pro achieved 100\% success rate after an average of 1.42 iterations per animation, demonstrating that current large language models with appropriate structural guidance can effectively automate mathematical animation creation.

\textbf{4. Efficiency Achievement:} Our system reduces animation creation time from 8-12 hours (traditional manual coding) to under 5 minutes (average 4.8 minutes), representing 96-98\% time reduction and fulfilling our goal of democratizing access to high-quality mathematical visualization tools.

\textbf{5. Educational Quality:} Expert educator evaluation (8 mathematics instructors) rated generated animations 4.4/5.0 overall, with particularly high scores for mathematical accuracy (4.8/5.0) and curriculum alignment (4.5/5.0), confirming pedagogical soundness.

\subsection{Contributions}

\textbf{1. Working System:} First demonstrated successful automated generation of complete Manim derivative animations without requiring programming expertise, addressing a significant educational accessibility barrier.

\textbf{2. Curated Dataset:} 537 expert-validated animations spanning four curriculum levels, publicly released for research and educational use.

\textbf{3. Technical Insights:} Empirical evidence that prompt engineering with large models outperforms fine-tuning small models for complex structured code generation, even with domain-specific training data—challenging assumptions about optimal approaches for specialized tasks.

\textbf{4. Design Principles:} Established template-guided generation and iterative refinement as effective strategies for reliable code generation in educational contexts.

\textbf{5. Educational Impact:} Practical tool enabling equitable access to professional-quality mathematical visualizations regardless of instructor technical background.

\subsection{Broader Implications}

Our findings have implications beyond derivative animation:

\textbf{For Educational Technology:} Demonstrates viability of AI-assisted content creation for reducing barriers to creating high-quality learning materials across STEM disciplines.

\textbf{For LLM Research:} Provides case study showing that scale + prompting can outperform specialization + fine-tuning for certain complex tasks, informing model selection decisions for practitioners.

\textbf{For Equity in Education:} Proves technical feasibility of democratizing advanced teaching tools, supporting broader efforts to ensure all students receive enhanced learning experiences regardless of institutional resources.

\subsection{Practical Recommendations}

For researchers and practitioners working on AI-assisted educational content creation:

\begin{enumerate}
\item \textbf{Prioritize scale over specialization:} For complex structured generation with limited training data (hundreds of examples), large models with prompt engineering likely outperform fine-tuned small models.

\item \textbf{Embrace template-guided generation:} Providing structural scaffolding dramatically improves reliability and enables systematic error recovery.

\item \textbf{Design for iteration:} Build workflows supporting refinement rather than expecting perfect first-generation outputs.

\item \textbf{Validate pedagogical quality:} Technical correctness is necessary but insufficient—involve domain experts in evaluating educational effectiveness.

\item \textbf{Consider infrastructure tradeoffs:} API-based solutions enable rapid prototyping; self-hosted large models provide long-term cost efficiency for production deployment.
\end{enumerate}

\subsection{Final Remarks}

By demonstrating that current large language models with appropriate prompt engineering can successfully automate mathematical animation creation, this work opens pathways toward broader educational equity in STEM education. While fine-tuning smaller models remains an important research direction, our findings suggest that for immediate practical impact, leveraging existing large models through careful prompt design may be the most viable approach.

We hope this research accelerates the development of AI-assisted educational tools that empower all mathematics educators—regardless of technical background—to create the dynamic visualizations that research has consistently shown enhance student learning outcomes. The publicly released dataset and insights from our comparative evaluation provide a foundation for future work toward this goal.

\subsection{Data and Code Availability}

Our complete dataset of 537 Manim derivative animations, prompt templates, and evaluation scripts are publicly available at: \texttt{https://github.com/Sushan-Adhikari/DerivativeAnimator}

\section*{Acknowledgments}

We thank the mathematics educators who participated in our pedagogical quality evaluation. We acknowledge the computational resources provided by Kathmandu University and lightning.ai for fine-tuning experiments. This research was conducted with the goal of improving educational equity in STEM education globally.

\begin{thebibliography}{99}

\bibitem{ref1} Mayer R E (2002) Multimedia learning \textit{Psychology of Learning and Motivation} \textbf{41} 85-139

\bibitem{ref2} Chen M et al (2021) Evaluating large language models trained on code \textit{arXiv:2107.03374}

\bibitem{ref3} Rozière B et al (2023) Code llama: Open foundation models for code \textit{arXiv:2308.12950}

\bibitem{ref4} Li R et al (2023) StarCoder: may the source be with you! \textit{arXiv:2305.06161}

\bibitem{ref5} Taori R et al (2023) Alpaca: A strong, replicable instruction-following model \textit{Stanford Center for Research on Foundation Models}

\bibitem{ref6} Hu E J et al (2021) LoRA: Low-Rank Adaptation of Large Language Models \textit{ICLR 2022}

\bibitem{ref7} DeepSeek-AI (2024) DeepSeek Coder: When the Large Language Model Meets Programming \textit{arXiv:2401.14196}

\bibitem{ref8} Ouyang L et al (2022) Training language models to follow instructions with human feedback \textit{arXiv:2203.02155}

\bibitem{ref9} Wei J et al (2022) Emergent abilities of large language models \textit{arXiv:2206.07682}

\bibitem{ref10} Austin J et al (2021) Program synthesis with large language models \textit{arXiv:2108.07732}

\bibitem{ref11} Li Y et al (2022) Competition-level code generation with AlphaCode \textit{Science} \textbf{378} 1092-1097

\bibitem{ref12} Wang Y et al (2021) CodeT5: Identifier-aware unified pre-trained encoder-decoder models \textit{EMNLP 2021}

\bibitem{ref13} Shin E C et al (2021) Constrained language models yield few-shot semantic parsers \textit{EMNLP 2019}

\bibitem{ref14} Xu F F et al (2022) A systematic evaluation of large language models of code \textit{ICLR Workshop 2022}

\bibitem{ref15} Fried D et al (2023) InCoder: A generative model for code infilling and synthesis \textit{ICLR 2023}

\bibitem{ref16} Manim Community (2023) Manim – Mathematical Animation Engine \texttt{https://www.manim.community/}

\bibitem{ref17} Hunter J D (2007) Matplotlib: A 2D graphics environment \textit{Computing in Science \& Engineering} \textbf{9} 90-95

\bibitem{ref18} Brown T B et al (2020) Language models are few-shot learners \textit{NeurIPS 2020}

\bibitem{ref19} Chowdhery A et al (2022) PaLM: Scaling language modeling with pathways \textit{arXiv:2204.02311}

\bibitem{ref20} Anil R et al (2023) Gemini: A family of highly capable multimodal models \textit{arXiv:2312.11805}

\end{thebibliography}

\end{document}
